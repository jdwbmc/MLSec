{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3\tEmploying Machine Learning in Security in the Real World\n",
    "## 4.3.4\tDeveloping a simple spam filter example\n",
    "Spam is any data you don't want to see. It could be part of an email. However, spam also occurs in documents, database, datasets, and all sort of other sources. Most people associate spam with emails and text, but spam could also be an image or an advertisement. The point is that you want to filter out unwanted data to keep machine learning applications safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STARTING WITH SIMPLE REMOVAL\n",
    "One of the more important tasks to perform is removing obviously bad data from a file or a group of files. For simplicity, this part of the example relies on a single file with multiple unwanted lines in it. These lines are defined by the word `bad`. However, you could just as easily look for other terms or characters used to create scripts, such as `{` and `}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Lines(filename, target_word):\n",
    "    useful_lines = []\n",
    "    with open(filename) as entries:\n",
    "        while True:\n",
    "            line = entries.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if not target_word.upper() in line.upper():\n",
    "                useful_lines += [line.rstrip()]\n",
    "    return useful_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've gained access to this file.\n",
      "This line is good.\n",
      "And, this line is just sort of OK.\n",
      "Finally, this line is great!\n"
     ]
    }
   ],
   "source": [
    "filename = 'TestAccess.txt'\n",
    "target = 'bad'\n",
    "\n",
    "good_data = Remove_Lines(filename, target)\n",
    "for entry in good_data:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MANIPULATING FILTERED DATA\n",
    "It's essential to know how to manipulate a cleaned dataset so that you can perform analysis on it. For example, seeing a word too often might tell you that there is something wrong with the data. As part of this process, you remove the stop words for the language you're using so that they don't add to the processing time. Stop words don't add value to the output of data manipulation. Creating a matrix of word usage in a dataset is also helpful. The matrix allows you to use other forms of analysis to look for word patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Stop_Words(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    new_lines = []\n",
    "    for line in data:\n",
    "        words = line.split()\n",
    "        filtered = [word for word in words if word.lower() not in stop_words]\n",
    "        new_lines += [' '.join(filtered)]\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Dictionary(data):\n",
    "    all_words = []\n",
    "    for line in data:\n",
    "        words = line.split()\n",
    "        all_words += words\n",
    "        \n",
    "    dictionary = Counter(all_words)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Features(data, dictionary):\n",
    "    features_matrix = np.zeros((len(data),len(dictionary)))\n",
    "    lineID = 0\n",
    "    for line in data:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "          wordID = 0\n",
    "          for i,d in enumerate(dictionary):\n",
    "            if d == word:\n",
    "              wordID = i\n",
    "              features_matrix[lineID, wordID] += 1\n",
    "        lineID += 1\n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You've gained access file.\", 'line good.', 'And, line sort OK.', 'Finally, line great!']\n",
      "Counter({'line': 3, \"You've\": 1, 'gained': 1, 'access': 1, 'file.': 1, 'good.': 1, 'And,': 1, 'sort': 1, 'OK.': 1, 'Finally,': 1, 'great!': 1})\n",
      "[[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "filtered = Remove_Stop_Words(good_data)\n",
    "print(filtered)\n",
    "\n",
    "word_dict = Create_Dictionary(filtered)\n",
    "print(word_dict)\n",
    "\n",
    "word_matrix = Extract_Features(filtered, word_dict)\n",
    "print(word_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING AN EMAIL FILTER\n",
    "Looking for spam within a group of emails is one of the tasks that machine learning does well. This example relies on the Ling-spam corpus described at https://www.kaggle.com/mandygu/lingspam-dataset. However, the original dataset is immense, so you instead use a specialized version from the `\\lingspam_public\\lingspam_public\\lemm_stop\\` folder, which provides the messages with the stop words already processed and the words normalized using _lemmatization_. The messages in `Email_Train` come from the `part1`, `part2`, and `part3` folders (867 messages total, with 144 spam messages), while the messages in the `Email_Test` folder come from the `part4` folder (289 messages total with 48 spam messages). You can tell which messages contain spam because they start with the letters `spmsg` (for spam message)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"Email_Train\"\n",
    "train_emails = \\\n",
    "    [os.path.join(train_path,f) for f in os.listdir(train_path)]\n",
    "\n",
    "test_path = \"Email_Test\"\n",
    "test_emails = \\\n",
    "    [os.path.join(test_path,f) for f in os.listdir(test_path)]\n",
    "\n",
    "def Create_Mail_Dictionary(emails):\n",
    "    all_words = []\n",
    "    for email in emails:\n",
    "        with open(email) as data:\n",
    "            for i,line in enumerate(data):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "\n",
    "    # Create the dictionary of all tokens within\n",
    "    # the entire list of emails.\n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    # Remove all of the non-word entries.\n",
    "    for item in [key for key in dictionary.keys() if key.isalpha() == False]:\n",
    "        del dictionary[item]\n",
    "    for item in [key for key in dictionary.keys() if len(key) == 1]:\n",
    "        del dictionary[item]\n",
    "            \n",
    "    # Return only the top 2000 words to reduce processing time.\n",
    "    dictionary = dictionary.most_common(2000)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "train_dict = Create_Mail_Dictionary(train_emails)\n",
    "test_dict = Create_Mail_Dictionary(test_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Mail_Features(emails, dictionary):\n",
    "    features_matrix = np.zeros((len(emails),len(dictionary)))\n",
    "    docID = 0\n",
    "    for email in emails:\n",
    "        with open(email) as data:\n",
    "            for i,line in enumerate(data):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                      wordID = 0\n",
    "                      for i,d in enumerate(dictionary):\n",
    "                        if d == word:\n",
    "                          wordID = i\n",
    "                          features_matrix[docID, wordID] = words.count(word)\n",
    "                    docID += 1\n",
    "    return features_matrix\n",
    "\n",
    "train_feat = Extract_Mail_Features(train_emails, train_dict)\n",
    "test_feat = Extract_Mail_Features(test_emails, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[241   0]\n",
      " [ 48   0]]\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.zeros(867)\n",
    "train_labels[723:867] = 1\n",
    "\n",
    "test_labels = np.zeros(289)\n",
    "test_labels[241:289] = 1\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(train_feat, train_labels)\n",
    "\n",
    "result = MNB.predict(test_feat)\n",
    "print(confusion_matrix(test_labels, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming versus Lemmatization\n",
    "There are two common techniques for normalizing words within documents: stemming and lemmatization. Each has its uses. _Stemmming_ simply removes the prefixes and suffixes of words to normalize on root word. For example: player, plays, and playing would all be stemmed to the root word play. This technique is most used for word analysis, such as determining how often particular words appear in one or more documents. _Lemmatization_ processes the words in context, so that the words running, runs, and ran all appear as the root word run. You use this technique most often for text analysis, such as determining the relationships of words in a spam message versus a usable (ham) message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "gary play the play piano whil play card .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "LS = LancasterStemmer()\n",
    "print(LS.stem(\"player\"))\n",
    "print(LS.stem(\"plays\"))\n",
    "print(LS.stem(\"playing\"))\n",
    "\n",
    "tokens = word_tokenize(\"Gary played the player piano while playing cards.\")\n",
    "stemmed = [LS.stem(word) for word in tokens]\n",
    "print(\" \".join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player\n",
      "play\n",
      "play\n",
      "Gary play the player piano while play card .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "WNL = WordNetLemmatizer()\n",
    "print(WNL.lemmatize(\"player\", pos=\"v\"))\n",
    "print(WNL.lemmatize(\"plays\", pos=\"v\"))\n",
    "print(WNL.lemmatize(\"playing\", pos=\"v\"))\n",
    "\n",
    "tokens = word_tokenize(\"Gary played the player piano while playing cards.\")\n",
    "lemmatized = [WNL.lemmatize(word, pos=\"v\") for word in tokens]\n",
    "print(\" \".join(lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
