{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for GPU Support\n",
    "It's helpful to know whether you have GPU support for creating models. Otherwise, it could take a very long time to complete the model. Trying to configure such a setup can prove time consuming and error prone. You need to know which versions of the CUDA Toolkit and cuDNN to install for your particular GPU. This process starts by determining the compute compatibility level of your GPU using the resource at https://developer.nvidia.com/cuda-gpus. For example, an NVidia GeForce GTX 760 (chosen for a specific reason in this example) has a compute capacity of 3.0.\n",
    "\n",
    "Once you determine the compute capacity, you must find a CUDA Toolkit version that will actually work, which means cross-referencing one to the other. Unfortunately, finding such information on the NVidia site is nearly impossible, so use the Wikipedia article at https://en.wikipedia.org/wiki/CUDA instead. Even though the article shows that a GPU with a compute capacity of 3.0 should work find with the 10.2 CUDA Toolkit version, in practice, it's actually better to use the 9.2 version, which you can download at https://developer.nvidia.com/cuda-toolkit-archive.\n",
    "\n",
    "Now you have a CUDA Toolkit in hand, so it's time to look for a compatible cuDNN version. You can find it at https://developer.nvidia.com/rdp/cudnn-archive.\n",
    "\n",
    "Creating a Anaconda Channel (environment) comes next. Before you assume that you can use the latest version of TensorFlow, which requires a compute compatibilty of 3.5, you might want to check around for articles like the one at https://medium.com/@chrzkan/install-tensorflow-gpu-with-cuda-9-2-driver-410-x-on-ubuntu-16-04-c4a5a09e0ace. In this case, you find out that you must use TensorFlow version 1.12 instead to support your older GPU. So, to create your environment in this case, you type **`conda create -n TF-GPU python=3.6 anaconda=2020.07 tensorflow-gpu=1.12`** at the Anaconda Prompt and press Enter. Of course, if you're not using this specific version of the CUDA Toolkit, then you use the versions needed for your GPU. Of note in this case is that TensorFlow 1.12 won't run on Python 3.8, so you must use version 3.6 instead. Once the installation is done, you can activate your new environment by typing **`conda activate TF-GPU`** at the Anaconda Prompt and press Enter.\n",
    "\n",
    "In addition, even though few sources tell you to do so, you must include the locations of all of these elements as part of your path statement, such as for a Windows system:\n",
    "\n",
    "`\n",
    "SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\bin;%PATH%\n",
    "SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\extras\\CUPTI\\lib64;%PATH%\n",
    "SET PATH=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\include;%PATH%\n",
    "SET PATH=C:\\tools\\cuda\\bin;%PATH%`\n",
    "\n",
    "Finally, at this point, you can type **`python`** at the Anaconda Prompt and press Enter to test some code. The following check determines if TensorFlow can actually see your GPU. It has to be able to see the GPU before it can use it. This particular check works for both TensorFlow 1.0 and 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3526765134495376882,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12058572457335283258\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3181458228\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12593673639982278101\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9506806888515202508\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 1.0\n",
    "The following code is designed to work with TensorFlow 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "# Show how the calculation is performed.\n",
    "print(c)\n",
    "\n",
    "# Show the calculation result.\n",
    "with tf.Session() as sess:\n",
    "    print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2.0\n",
    "The following code is designed to work with TensorFlow 2.0. It won't work with TensorFlow 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
